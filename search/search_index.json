{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pupil Labs Video","text":"<p>A high-level wrapper of PyAV providing an easy to use interface to video data.</p> <p>The goal of this library is to provide a simple interface while maintaining good computational performance. At current, only MP4 and MJPEG videos are officially compatible.</p> <p>Features include:</p> <ul> <li>Performant reading of video files (optionally including audio) utilizing multi-threading.</li> <li>Ability to arbitrarily index frames by their index or timestamp.</li> <li>Ability to slice the video by frame index or time. Large slices will be loaded lazily to avoidexcessive RAM consumption.</li> <li>A frame buffer is maintained to cache frames close to the current decoding position, which avoids repetitive seeking when going back and forth between frames in the same neighborhood or iterating backwards.</li> <li>Avoids demuxing and seeking operations as much as possible.</li> <li>Reading multi-part video files (e.g. how they are generated by Neon or Pupil Invisible).</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pupil-labs-video\n</code></pre> <p>or</p> <pre><code>pip install -e git+https://github.com/pupil-labs/pl-video.git\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>You can open a video file and read frames like this:</p> <pre><code>import pupil_labs.video as plv\n\nwith plv.Reader(video_path) as video:\n    # Iterate through video frames\n    for frame in video:\n        # Convert video frame to BGR array\n        img = frame.bgr\n\n    # Index individual frames or slices\n    first_frame = video[0]\n    last_frame = video[-1]\n    frames = video[10:20]\n\n    # Index frames by time\n    ts = video[10].time\n    frame = video.by_container_timestamps[ts]\n    frames = video.by_container_timestamps[ts : ts + 10]\n</code></pre> <p>You can write video files like this:</p> <pre><code>import pupil_labs.video as plv\n\nwith (plv.Writer(out_path) as writer):\n    for img in images:\n        writer.write_image(img)\n</code></pre>"},{"location":"contributing/","title":"Developer","text":""},{"location":"license/","title":"License","text":"<pre><code>---\ntitle: MIT License\nspdx-id: MIT\nfeatured: true\nhidden: false\n\ndescription: A short and simple permissive license with conditions only requiring preservation of copyright and license notices. Licensed works, modifications, and larger works may be distributed under different terms and without source code.\n\nhow: Create a text file (typically named LICENSE or LICENSE.txt) in the root of your source code and copy the text of the license into the file. Replace [year] with the current year and [fullname] with the name (or names) of the copyright holders.\n\nusing:\n  Babel: https://github.com/babel/babel/blob/master/LICENSE\n  .NET: https://github.com/dotnet/runtime/blob/main/LICENSE.TXT\n  Rails: https://github.com/rails/rails/blob/master/MIT-LICENSE\n\npermissions:\n  - commercial-use\n  - modifications\n  - distribution\n  - private-use\n\nconditions:\n  - include-copyright\n\nlimitations:\n  - liability\n  - warranty\n\n---\n\nMIT License\n\nCopyright (c) 2025 Pupil Labs GmbH\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"modules/","title":"API reference","text":""},{"location":"modules/#pupil_labs.video","title":"video","text":"<p>pupil_labs.video package.</p> <p>A high-level wrapper of PyAV providing an easy to use interface to video data.</p> <p>Modules:</p> <ul> <li> <code>frame</code>           \u2013            </li> <li> <code>indexing</code>           \u2013            </li> <li> <code>reader</code>           \u2013            </li> <li> <code>writer</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>AudioFrame</code>           \u2013            </li> <li> <code>Reader</code>           \u2013            </li> <li> <code>VideoFrame</code>           \u2013            </li> <li> <code>Writer</code>           \u2013            </li> </ul>"},{"location":"modules/#pupil_labs.video.AudioFrame","title":"AudioFrame  <code>dataclass</code>","text":"<pre><code>AudioFrame(av_frame: AudioFrame, time: float, index: int, source: Any)\n</code></pre> <p>               Bases: <code>BaseFrame</code></p> <p>Methods:</p> <ul> <li> <code>to_ndarray</code>             \u2013              <p>Convert the audio samples of the AudioFrame to a numpy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>AudioFrame</code>)           \u2013            <p>the original av.AudioFrame for this frame</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>index of frame</p> </li> <li> <code>source</code>               (<code>Any</code>)           \u2013            <p>source of this frame, eg. reader or filename</p> </li> <li> <code>time</code>               (<code>float</code>)           \u2013            <p>timestamp of frame</p> </li> </ul>"},{"location":"modules/#pupil_labs.video.AudioFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: AudioFrame\n</code></pre> <p>the original av.AudioFrame for this frame</p>"},{"location":"modules/#pupil_labs.video.AudioFrame.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>index of frame</p>"},{"location":"modules/#pupil_labs.video.AudioFrame.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source: Any\n</code></pre> <p>source of this frame, eg. reader or filename</p>"},{"location":"modules/#pupil_labs.video.AudioFrame.time","title":"time  <code>instance-attribute</code>","text":"<pre><code>time: float\n</code></pre> <p>timestamp of frame</p>"},{"location":"modules/#pupil_labs.video.AudioFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray() -&gt; NDArray[float64]\n</code></pre> <p>Convert the audio samples of the AudioFrame to a numpy array.</p> Source code in <code>src/pupil_labs/video/frame.py</code> <pre><code>def to_ndarray(self) -&gt; npt.NDArray[np.float64]:\n    \"\"\"Convert the audio samples of the AudioFrame to a numpy array.\"\"\"\n    return cast(npt.NDArray[np.float64], self.av_frame.to_ndarray())\n</code></pre>"},{"location":"modules/#pupil_labs.video.Reader","title":"Reader","text":"<pre><code>Reader(source: Path | str, stream: Literal['video'] = 'video', container_timestamps: Optional[ContainerTimestamps | list[float]] | None = None, logger: Logger | None = None)\n</code></pre><pre><code>Reader(source: Path | str, stream: Literal['audio'] = 'audio', container_timestamps: Optional[ContainerTimestamps | list[float]] | None = None, logger: Logger | None = None)\n</code></pre> <pre><code>Reader(source: Path | str, stream: Literal['audio', 'video'] | tuple[Literal['audio', 'video'], int] = 'video', container_timestamps: Optional[ContainerTimestamps | list[float]] | None = None, logger: Optional[Logger] = None)\n</code></pre> <p>               Bases: <code>Generic[ReaderFrameType]</code></p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Path | str</code>)           \u2013            <p>Path to a video file. Can be a local path or an http-address.</p> </li> <li> <code>stream</code>               (<code>Literal['audio', 'video'] | tuple[Literal['audio', 'video'], int]</code>, default:                   <code>'video'</code> )           \u2013            <p>The stream to read from, either \"audio\", \"video\". If the video file contains multiple streams of the deisred kind, a tuple can be provided to specify which stream to use, e.g. <code>(\"audio\", 2)</code> to use the audio stream at index <code>2</code>.</p> </li> <li> <code>container_timestamps</code>               (<code>Optional[ContainerTimestamps | list[float]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Array containing the timestamps of the video frames in container time (equal to PTS * time_base). If not provided, timestamps will be inferred from the container. Providing pre-loaded values can speed up initialization for long videos by avoiding demuxing of the entire video to obtain PTS.</p> </li> <li> <code>logger</code>               (<code>Optional[Logger]</code>, default:                   <code>None</code> )           \u2013            <p>Python logger to use. Decreases performance.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>audio</code>               (<code>Reader[AudioFrame] | None</code>)           \u2013            <p>Returns an <code>Reader</code> providing access to the audio data of the video only.</p> </li> <li> <code>average_rate</code>               (<code>float</code>)           \u2013            <p>Return the average framerate of the video in Hz.</p> </li> <li> <code>by_container_timestamps</code>               (<code>Indexer[ReaderFrameType]</code>)           \u2013            <p>Time-based access to video frames using container timestamps.</p> </li> <li> <code>container_timestamps</code>               (<code>ContainerTimestamps</code>)           \u2013            <p>Frame timestamps in container time.</p> </li> <li> <code>duration</code>               (<code>float</code>)           \u2013            <p>Return the duration of the video in seconds.</p> </li> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>Return the filename of the video</p> </li> <li> <code>gop_size</code>               (<code>int</code>)           \u2013            <p>Return the amount of frames per keyframe in a video</p> </li> <li> <code>height</code>               (<code>int | None</code>)           \u2013            <p>Height of the video in pixels.</p> </li> <li> <code>pts</code>               (<code>list[int]</code>)           \u2013            <p>Return all presentation timestamps in <code>video.time_base</code></p> </li> <li> <code>rate</code>               (<code>Fraction | int | None</code>)           \u2013            <p>Return the framerate of the video in Hz.</p> </li> <li> <code>source</code>               (<code>Any</code>)           \u2013            <p>Return the source of the video</p> </li> <li> <code>video</code>               (<code>Reader[VideoFrame] | None</code>)           \u2013            <p>Returns an <code>Reader</code> providing access to the video data of the video only.</p> </li> <li> <code>width</code>               (<code>int | None</code>)           \u2013            <p>Width of the video in pixels.</p> </li> </ul> Source code in <code>src/pupil_labs/video/reader.py</code> <pre><code>def __init__(\n    self,\n    source: Path | str,\n    stream: Literal[\"audio\", \"video\"]\n    | tuple[Literal[\"audio\", \"video\"], int] = \"video\",\n    container_timestamps: Optional[ContainerTimestamps | list[float]] | None = None,\n    logger: Optional[Logger] = None,\n):\n    \"\"\"Create a reader for a video file.\n\n    Args:\n        source: Path to a video file. Can be a local path or an http-address.\n        stream: The stream to read from, either \"audio\", \"video\". If the video file\n            contains multiple streams of the deisred kind, a tuple can be provided\n            to specify which stream to use, e.g. `(\"audio\", 2)` to use the audio\n            stream at index `2`.\n        container_timestamps: Array containing the timestamps of the video frames in\n            container time (equal to PTS * time_base). If not provided, timestamps\n            will be inferred from the container. Providing pre-loaded values can\n            speed up initialization for long videos by avoiding demuxing of the\n            entire video to obtain PTS.\n        logger: Python logger to use. Decreases performance.\n\n    \"\"\"\n    self._container_timestamps: ContainerTimestamps | None = None\n    if container_timestamps is not None:\n        if isinstance(container_timestamps, list):\n            container_timestamps = np.array(container_timestamps)\n        self.container_timestamps = container_timestamps\n\n    self.lazy_frame_slice_limit = LAZY_FRAME_SLICE_LIMIT\n    self._times_were_provided = container_timestamps is not None\n    self._source = source\n    self._logger = logger or DEFAULT_LOGGER\n    self.stats = Stats()\n\n    if not isinstance(stream, tuple):\n        stream = (stream, 0)\n    self._stream_kind, self._stream_index = stream\n\n    self._log = bool(logger)\n    self._is_at_start = True\n    self._last_processed_dts = -maxsize\n    self._partial_pts = list[int]()\n    self._partial_dts = list[int]()\n    self._partial_pts_to_index = dict[int, int]()\n    self._all_pts_are_loaded = False\n    self._decoder_frame_buffer = deque[AVFrame]()\n    self._current_decoder_index: int | None = -1\n    self._indexed_frames_buffer: deque[ReaderFrameType] = deque(maxlen=1000)\n    # TODO(dan): can we avoid it?\n    # this forces loading the gopsize on initialization to set the buffer length\n    assert self.gop_size\n</code></pre>"},{"location":"modules/#pupil_labs.video.Reader.audio","title":"audio  <code>cached</code> <code>property</code>","text":"<pre><code>audio: Reader[AudioFrame] | None\n</code></pre> <p>Returns an <code>Reader</code> providing access to the audio data of the video only.</p>"},{"location":"modules/#pupil_labs.video.Reader.average_rate","title":"average_rate  <code>property</code>","text":"<pre><code>average_rate: float\n</code></pre> <p>Return the average framerate of the video in Hz.</p>"},{"location":"modules/#pupil_labs.video.Reader.by_container_timestamps","title":"by_container_timestamps  <code>cached</code> <code>property</code>","text":"<pre><code>by_container_timestamps: Indexer[ReaderFrameType]\n</code></pre> <p>Time-based access to video frames using container timestamps.</p> <p>Container time is measured in seconds relative to begining of the video. Accordingly, the first frame typically has timestamp <code>0.0</code>.</p> <p>When accessing a specific key, e.g. <code>reader[t]</code>, a frame with this exact timestamp needs to exist, otherwise an <code>IndexError</code> is raised. When acessing a slice, e.g. <code>reader[a:b]</code> an <code>ArrayLike</code> is returned such that <code>a &lt;= frame.time &lt; b</code> for every frame.</p> <p>Large slices are returned as a lazy view, which avoids immediately loading all frames into RAM.</p> <p>Note that numerical imprecisions of float numbers can lead to issues when accessing individual frames by their container timestamp. It is recommended to prefer indexing frames via slices.</p>"},{"location":"modules/#pupil_labs.video.Reader.container_timestamps","title":"container_timestamps  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<pre><code>container_timestamps: ContainerTimestamps\n</code></pre> <p>Frame timestamps in container time.</p> <p>Container time is measured in seconds relative to begining of the video. Accordingly, the first frame typically has timestamp <code>0.0</code>.</p> <p>If these values were not provided when creating the Reader, they will be inferred from the video container.</p>"},{"location":"modules/#pupil_labs.video.Reader.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>Return the duration of the video in seconds.</p> <p>If the duration is not available in the container, it will be calculated based on the frames timestamps.</p>"},{"location":"modules/#pupil_labs.video.Reader.filename","title":"filename  <code>property</code>","text":"<pre><code>filename: str\n</code></pre> <p>Return the filename of the video</p>"},{"location":"modules/#pupil_labs.video.Reader.gop_size","title":"gop_size  <code>cached</code> <code>property</code>","text":"<pre><code>gop_size: int\n</code></pre> <p>Return the amount of frames per keyframe in a video</p>"},{"location":"modules/#pupil_labs.video.Reader.height","title":"height  <code>property</code>","text":"<pre><code>height: int | None\n</code></pre> <p>Height of the video in pixels.</p>"},{"location":"modules/#pupil_labs.video.Reader.pts","title":"pts  <code>cached</code> <code>property</code>","text":"<pre><code>pts: list[int]\n</code></pre> <p>Return all presentation timestamps in <code>video.time_base</code></p>"},{"location":"modules/#pupil_labs.video.Reader.rate","title":"rate  <code>property</code>","text":"<pre><code>rate: Fraction | int | None\n</code></pre> <p>Return the framerate of the video in Hz.</p>"},{"location":"modules/#pupil_labs.video.Reader.source","title":"source  <code>property</code>","text":"<pre><code>source: Any\n</code></pre> <p>Return the source of the video</p>"},{"location":"modules/#pupil_labs.video.Reader.video","title":"video  <code>cached</code> <code>property</code>","text":"<pre><code>video: Reader[VideoFrame] | None\n</code></pre> <p>Returns an <code>Reader</code> providing access to the video data of the video only.</p>"},{"location":"modules/#pupil_labs.video.Reader.width","title":"width  <code>property</code>","text":"<pre><code>width: int | None\n</code></pre> <p>Width of the video in pixels.</p>"},{"location":"modules/#pupil_labs.video.VideoFrame","title":"VideoFrame  <code>dataclass</code>","text":"<pre><code>VideoFrame(av_frame: VideoFrame, time: float, index: int, source: Any)\n</code></pre> <p>               Bases: <code>BaseFrame</code></p> <p>Methods:</p> <ul> <li> <code>to_ndarray</code>             \u2013              <p>Convert the image of the VideoFrame to a numpy array.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>av_frame</code>               (<code>VideoFrame</code>)           \u2013            <p>the original av.VideoFrame for this frame</p> </li> <li> <code>bgr</code>               (<code>NDArray[uint8]</code>)           \u2013            <p>Numpy image array in BGR format</p> </li> <li> <code>gray</code>               (<code>NDArray[uint8]</code>)           \u2013            <p>Numpy image array in gray format</p> </li> <li> <code>index</code>               (<code>int</code>)           \u2013            <p>index of frame</p> </li> <li> <code>rgb</code>               (<code>NDArray[uint8]</code>)           \u2013            <p>Numpy image array in RGB format</p> </li> <li> <code>source</code>               (<code>Any</code>)           \u2013            <p>source of this frame, eg. reader or filename</p> </li> <li> <code>time</code>               (<code>float</code>)           \u2013            <p>timestamp of frame</p> </li> </ul>"},{"location":"modules/#pupil_labs.video.VideoFrame.av_frame","title":"av_frame  <code>instance-attribute</code>","text":"<pre><code>av_frame: VideoFrame\n</code></pre> <p>the original av.VideoFrame for this frame</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.bgr","title":"bgr  <code>property</code>","text":"<pre><code>bgr: NDArray[uint8]\n</code></pre> <p>Numpy image array in BGR format</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.gray","title":"gray  <code>property</code>","text":"<pre><code>gray: NDArray[uint8]\n</code></pre> <p>Numpy image array in gray format</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.index","title":"index  <code>instance-attribute</code>","text":"<pre><code>index: int\n</code></pre> <p>index of frame</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.rgb","title":"rgb  <code>property</code>","text":"<pre><code>rgb: NDArray[uint8]\n</code></pre> <p>Numpy image array in RGB format</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source: Any\n</code></pre> <p>source of this frame, eg. reader or filename</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.time","title":"time  <code>instance-attribute</code>","text":"<pre><code>time: float\n</code></pre> <p>timestamp of frame</p>"},{"location":"modules/#pupil_labs.video.VideoFrame.to_ndarray","title":"to_ndarray","text":"<pre><code>to_ndarray(pixel_format: PixelFormat) -&gt; NDArray[uint8]\n</code></pre> <p>Convert the image of the VideoFrame to a numpy array.</p> Source code in <code>src/pupil_labs/video/frame.py</code> <pre><code>def to_ndarray(self, pixel_format: PixelFormat) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"Convert the image of the VideoFrame to a numpy array.\"\"\"\n    # TODO: add caching for decoded frames?\n    return av_frame_to_ndarray_fast(self.av_frame, pixel_format)\n</code></pre>"},{"location":"modules/#pupil_labs.video.Writer","title":"Writer","text":"<pre><code>Writer(path: str | Path, lossless: bool = False, fps: int | None = None, bit_rate: int = 2000000, logger: Logger | None = None)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | Path</code>)           \u2013            <p>The path to write the video to.</p> </li> <li> <code>lossless</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the video will be encoded in lossless H264.</p> </li> <li> <code>fps</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The desired framerate of the video.</p> </li> <li> <code>bit_rate</code>               (<code>int</code>, default:                   <code>2000000</code> )           \u2013            <p>The desired bit rate of the video.</p> </li> <li> <code>logger</code>               (<code>Logger | None</code>, default:                   <code>None</code> )           \u2013            <p>Python logger to use. Decreases performance.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>write_image</code>             \u2013              <p>Write an image to the video.</p> </li> </ul> Source code in <code>src/pupil_labs/video/writer.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    lossless: bool = False,\n    fps: int | None = None,\n    bit_rate: int = 2_000_000,\n    logger: Logger | None = None,\n) -&gt; None:\n    \"\"\"Video writer for creating videos from image arrays.\n\n    Args:\n        path: The path to write the video to.\n        lossless: If True, the video will be encoded in lossless H264.\n        fps: The desired framerate of the video.\n        bit_rate: The desired bit rate of the video.\n        logger: Python logger to use. Decreases performance.\n\n    \"\"\"\n    self.path = path\n    self.lossless = lossless\n    self.fps = fps\n    self.bit_rate = bit_rate\n    self.logger = logger or DEFAULT_LOGGER\n    self.container = av.open(self.path, \"w\")\n</code></pre>"},{"location":"modules/#pupil_labs.video.Writer.write_image","title":"write_image","text":"<pre><code>write_image(image: NDArray[uint8], time: Optional[float] = None, pix_fmt: Optional[PixelFormat] = None) -&gt; None\n</code></pre> <p>Write an image to the video.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>NDArray[uint8]</code>)           \u2013            <p>The image to write. Can have 1 or 3 channels.</p> </li> <li> <code>time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The time of the frame in seconds.</p> </li> <li> <code>pix_fmt</code>               (<code>Optional[PixelFormat]</code>, default:                   <code>None</code> )           \u2013            <p>The pixel format of the image. If None, the pixel format will be <code>gray</code> for 1-channel images and <code>bgr24</code> for 3-channel images.</p> </li> </ul> Source code in <code>src/pupil_labs/video/writer.py</code> <pre><code>def write_image(\n    self,\n    image: npt.NDArray[np.uint8],\n    time: Optional[float] = None,\n    pix_fmt: Optional[PixelFormat] = None,\n) -&gt; None:\n    \"\"\"Write an image to the video.\n\n    Args:\n        image: The image to write. Can have 1 or 3 channels.\n        time: The time of the frame in seconds.\n        pix_fmt: The pixel format of the image. If None, the pixel format will be\n            `gray` for 1-channel images and `bgr24` for 3-channel images.\n\n    \"\"\"\n    if pix_fmt is None:\n        pix_fmt = \"bgr24\"\n        if image.ndim == 2:\n            pix_fmt = \"gray\"\n\n    frame = av.VideoFrame.from_ndarray(image, str(pix_fmt))\n    self.write_frame(frame, time=time)\n</code></pre>"},{"location":"coverage/","title":"Coverage report","text":""}]}